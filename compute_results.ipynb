{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model evaluation results and compute accuracies, confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# add to path\n",
    "sys.path.append('evaluation')\n",
    "\n",
    "import benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bootstrap\n",
    "from sklearn import metrics\n",
    "\n",
    "def accuracy(labels, predictions, verbose=True, confidence_level=0.9):\n",
    "    \"\"\"Compute the accuracy. Also compute a confidence interval using a bootstrap method.\"\"\"\n",
    "    acc = metrics.accuracy_score(labels, predictions)\n",
    "    res = bootstrap(\n",
    "        (np.array(labels), np.array(predictions)),\n",
    "        metrics.accuracy_score,\n",
    "        vectorized=False,\n",
    "        paired=True,\n",
    "        confidence_level=confidence_level,\n",
    "        batch=min(len(labels), 5000),\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\n",
    "            f\"Accuracy: {acc:.3f}, {confidence_level*100:-2f}%-Confidence Interval: ({res.confidence_interval.low:.3f}, {res.confidence_interval.high:.3f}), Standard error: {res.standard_error:.3f}\"\n",
    "        )\n",
    "    return acc, res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of all files in the folder\n",
    "experiment_folder = \"results/124M_8x\"\n",
    "\n",
    "files = os.listdir(experiment_folder)\n",
    "\n",
    "# filter for json files\n",
    "files = [file for file in files if file.endswith(\".parquet\") and 'step' in file]\n",
    "\n",
    "# sort files by the number in the filename\n",
    "files = sorted(files, key=lambda x: int(x.split(\"=\")[1].split(\".\")[0]))\n",
    "\n",
    "# get the steps from the file names\n",
    "steps = [int(file.split(\"=\")[1].split(\".\")[0]) for file in files]\n",
    "\n",
    "# sort both steps and files\n",
    "steps, files = zip(*sorted(zip(steps, files)))\n",
    "\n",
    "print(files)\n",
    "print(steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the all contamination splits\n",
    "contamination_ds = benchmarks.load_benchmark('all-contamination-splits')\n",
    "contamination_ds = benchmarks.sort_length(contamination_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "# for each step, load the respective file\n",
    "for idx, step in enumerate(steps):\n",
    "    results[step] = {}\n",
    "    filename = files[idx]\n",
    "    print(f'file: {filename}')\n",
    "    # load the json dataset\n",
    "    ds = Dataset.from_parquet(os.path.join(experiment_folder, filename), keep_in_memory=True)\n",
    "    # sort dataset by option length (required for match with contamination_ds below)\n",
    "    ds = benchmarks.sort_length(ds)\n",
    "    # convert the dataset to a list of lists\n",
    "    ds = ds.to_pandas()\n",
    "    # add split-id column to the huggingface datasets object\n",
    "    ds['split-id'] = contamination_ds['split-id']\n",
    "    ds['benchmark'] = contamination_ds['benchmark']\n",
    "    # sanity check that we have matched the right split-id\n",
    "    for idx in tqdm(range(len(contamination_ds))):\n",
    "        assert ds.iloc[idx]['options'][0] == contamination_ds[idx]['options'][0] # check that the benchmark questions at the same index are the same\n",
    "    # filter by the value in the split-id column\n",
    "    values = np.unique(ds[\"split-id\"])\n",
    "    for v in values:\n",
    "        # select the values where split-id has the value\n",
    "        split_ds = ds[ds[\"split-id\"] == v]\n",
    "        # print the number of samples in the split\n",
    "        print(f\"Split {v}: {len(split_ds)} samples\")\n",
    "        # report the accuracy\n",
    "        split_acc, ce = accuracy(np.array(split_ds['label'].values), split_ds['prediction'].values, confidence_level=0.90)\n",
    "        results[step][v] = (split_acc, ce) \n",
    "        # cross-entropy loss\n",
    "        ce_losses = []\n",
    "        for idx in range(len(split_ds)):\n",
    "            label_ce_loss = split_ds.iloc[idx]['ce_loss'][split_ds.iloc[idx]['label']]\n",
    "            ce_losses.append(np.nanmean(label_ce_loss))\n",
    "        split_ce_loss = np.nanmean(ce_losses)\n",
    "        # the 95% confidence interval of the cross-entropy loss\n",
    "        ce = bootstrap(\n",
    "            (np.array(ce_losses),),\n",
    "            np.nanmean,\n",
    "            vectorized=False,\n",
    "            paired=False,\n",
    "            confidence_level=0.90,\n",
    "        )\n",
    "        # print ce loss and 95% confidence interval\n",
    "        print(\n",
    "            f\"Cross-Entropy Loss: {split_ce_loss:.3f}, 90%-Confidence Interval: ({ce.confidence_interval.low:.3f}, {ce.confidence_interval.high:.3f}), Standard error: {ce.standard_error:.3f}\"\n",
    "        )\n",
    "        results[step][100+v] = (split_ce_loss, ce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "with open(f'results/cache/{os.path.basename(experiment_folder)}.pkl', 'wb') as f:\n",
    "    pkl.dump(results, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmcd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
