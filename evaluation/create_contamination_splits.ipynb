{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "import benchmarks\n",
    "\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "\n",
    "# auto-reload\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the different datasets and run duplicate detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hellaswag = Dataset.from_parquet(\"de-duplication/hellaswag_duplicates.parquet\")\n",
    "arc_easy = Dataset.from_parquet(\"de-duplication/arc_easy_duplicates.parquet\")\n",
    "boolq = Dataset.from_parquet(\"de-duplication/boolq_duplicates.parquet\")\n",
    "social_i_qa = Dataset.from_parquet(\"de-duplication/social_i_qa_duplicates.parquet\")\n",
    "piqa = Dataset.from_parquet(\"de-duplication/piqa_duplicates.parquet\")\n",
    "mmlu = Dataset.from_parquet(\"de-duplication/mmlu_duplicates.parquet\")\n",
    "winogrande = Dataset.from_parquet(\"de-duplication/winogrande_duplicates.parquet\")\n",
    "\n",
    "arc_easy = arc_easy.map(lambda x: {\"benchmark\": \"arc-easy\"})\n",
    "boolq = boolq.map(lambda x: {\"benchmark\": \"boolq\"})\n",
    "hellaswag = hellaswag.map(lambda x: {\"benchmark\": \"hellaswag\"})\n",
    "social_i_qa = social_i_qa.map(lambda x: {\"benchmark\": \"social_i_qa\"})\n",
    "piqa = piqa.map(lambda x: {\"benchmark\": \"piqa\"})\n",
    "mmlu = mmlu.map(lambda x: {\"benchmark\": \"mmlu\"})\n",
    "winogrande = winogrande.map(lambda x: {\"benchmark\": \"winogrande\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arc_easy), len(boolq), len(hellaswag), len(social_i_qa), len(piqa), len(mmlu), len(winogrande)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset, report the numbef of duplicates\n",
    "print(\"hellaswag\", len(hellaswag), np.sum(hellaswag[\"is_duplicate\"]))\n",
    "print(\"arc_easy\", len(arc_easy), np.sum(arc_easy[\"is_duplicate\"]))\n",
    "print(\"boolq\", len(boolq), np.sum(boolq[\"is_duplicate\"]))\n",
    "print(\"social_i_qa\", len(social_i_qa), np.sum(social_i_qa[\"is_duplicate\"]))\n",
    "print(\"piqa\", len(piqa), np.sum(piqa[\"is_duplicate\"]))\n",
    "print(\"mmlu\", len(mmlu), np.sum(mmlu[\"is_duplicate\"]))\n",
    "print(\"winogrande\", len(winogrande), np.sum(winogrande[\"is_duplicate\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each dataset, drop the duplicates\n",
    "arc_easy = arc_easy.filter(lambda x: not x[\"is_duplicate\"])\n",
    "boolq = boolq.filter(lambda x: not x[\"is_duplicate\"])\n",
    "hellaswag = hellaswag.filter(lambda x: not x[\"is_duplicate\"])\n",
    "social_i_qa = social_i_qa.filter(lambda x: not x[\"is_duplicate\"])\n",
    "piqa = piqa.filter(lambda x: not x[\"is_duplicate\"])\n",
    "mmlu = mmlu.filter(lambda x: not x[\"is_duplicate\"])\n",
    "winogrande = winogrande.filter(lambda x: not x[\"is_duplicate\"])\n",
    "\n",
    "# drop the duplicate columns\n",
    "arc_easy = arc_easy.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "boolq = boolq.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "hellaswag = hellaswag.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "social_i_qa = social_i_qa.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "piqa = piqa.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "mmlu = mmlu.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "winogrande = winogrande.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "\n",
    "len(arc_easy), len(boolq), len(hellaswag), len(social_i_qa), len(piqa), len(mmlu), len(winogrande)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# piqa formatting issue\n",
    "piqa = piqa.cast_column(\"label\", boolq.features['label'])\n",
    "piqa.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load result of de-duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = Dataset.from_parquet(\"de-duplication/all_datasets.parquet\")\n",
    "\n",
    "arc_easy = all_datasets.filter(lambda x: x[\"benchmark\"] == \"arc-easy\")\n",
    "boolq = all_datasets.filter(lambda x: x[\"benchmark\"] == \"boolq\")\n",
    "hellaswag = all_datasets.filter(lambda x: x[\"benchmark\"] == \"hellaswag\")\n",
    "social_i_qa = all_datasets.filter(lambda x: x[\"benchmark\"] == \"social_i_qa\")\n",
    "piqa = all_datasets.filter(lambda x: x[\"benchmark\"] == \"piqa\")\n",
    "mmlu = all_datasets.filter(lambda x: x[\"benchmark\"] == \"mmlu\")\n",
    "winogrande = all_datasets.filter(lambda x: x[\"benchmark\"] == \"winogrande\")\n",
    "\n",
    "# print duplicates per dataset\n",
    "print(\"arc_easy\", len(arc_easy), np.sum(arc_easy[\"is_duplicate\"]))\n",
    "print(\"boolq\", len(boolq), np.sum(boolq[\"is_duplicate\"]))\n",
    "print(\"hellaswag\", len(hellaswag), np.sum(hellaswag[\"is_duplicate\"]))\n",
    "print(\"social_i_qa\", len(social_i_qa), np.sum(social_i_qa[\"is_duplicate\"]))\n",
    "print(\"piqa\", len(piqa), np.sum(piqa[\"is_duplicate\"]))\n",
    "print(\"mmlu\", len(mmlu), np.sum(mmlu[\"is_duplicate\"]))\n",
    "print(\"winogrande\", len(winogrande), np.sum(winogrande[\"is_duplicate\"]))\n",
    "\n",
    "# drop the duplicates\n",
    "arc_easy = arc_easy.filter(lambda x: not x[\"is_duplicate\"])\n",
    "boolq = boolq.filter(lambda x: not x[\"is_duplicate\"])\n",
    "hellaswag = hellaswag.filter(lambda x: not x[\"is_duplicate\"])\n",
    "social_i_qa = social_i_qa.filter(lambda x: not x[\"is_duplicate\"])\n",
    "piqa = piqa.filter(lambda x: not x[\"is_duplicate\"])\n",
    "mmlu = mmlu.filter(lambda x: not x[\"is_duplicate\"])\n",
    "winogrande = winogrande.filter(lambda x: not x[\"is_duplicate\"])\n",
    "\n",
    "# drop the duplicate columns\n",
    "arc_easy = arc_easy.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "boolq = boolq.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "hellaswag = hellaswag.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "social_i_qa = social_i_qa.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "piqa = piqa.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "mmlu = mmlu.remove_columns([\"is_duplicate\", \"has_duplicate\"])\n",
    "winogrande = winogrande.remove_columns([\"is_duplicate\", \"has_duplicate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(arc_easy), len(boolq), len(hellaswag), len(social_i_qa), len(piqa), len(mmlu), len(winogrande), len(arc_easy) + len(boolq) + len(hellaswag) + len(social_i_qa) + len(piqa) + len(mmlu) + len(winogrande)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop observations from the largest datasets\n",
    "winogrande = winogrande.shuffle(seed=42).select(range(8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_observations = sum([len(arc_easy), len(boolq), len(hellaswag), len(social_i_qa), len(piqa), len(mmlu), len(winogrande)])\n",
    "total_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all datasets\n",
    "all_datasets = concatenate_datasets([arc_easy, boolq, hellaswag, social_i_qa, piqa, mmlu, winogrande])\n",
    "len(all_datasets)\n",
    "\n",
    "# save this to perform fuzzy string matching between the different questions on the cluster\n",
    "all_datasets.to_parquet('concatenated_datasets.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for each benchmark, report the random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ben_ds in [arc_easy, boolq, hellaswag, social_i_qa, piqa, mmlu, winogrande]:\n",
    "    print(ben_ds[0][\"benchmark\"])\n",
    "    print(benchmarks.random_baseline(ben_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup the schedule and the split the different datasets into parts of the needed size, stratified by accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the schedule that we use for the contamination\n",
    "schedule = [(0, 10000), \n",
    "            (4, 8000),\n",
    "            (12, 5000), \n",
    "            (36, 2000), \n",
    "            (144, 2000), \n",
    "            (4, 8000), \n",
    "            (12, 5000), \n",
    "            (36, 2000),\n",
    "            (144, 2000)] \n",
    "\n",
    "total_samples = np.sum([v for _,v in schedule])\n",
    "total_samples, np.sum([k*v for k,v in schedule]), len(schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the percentages of the different schedule items of the total items\n",
    "percentages = np.array([v for _,v in schedule])/total_samples\n",
    "\n",
    "percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [arc_easy, boolq, hellaswag, social_i_qa, piqa, mmlu, winogrande]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly sample from all datasets to reduce the overall size to total_samples\n",
    "# all_datasets = [dataset.shuffle(seed=42).select(range(int(len(dataset) * total_samples / total_observations))) for dataset in all_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we split each benchmark in different parts according to percentage sizes of the total\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_dataset(dataset, percentages, debug=False, random_state=42):\n",
    "    splits = []\n",
    "    remaining_percentage = 1.0\n",
    "    for per_cent in percentages:\n",
    "        # if we have the last element, we take the rest\n",
    "        if abs(remaining_percentage - per_cent) < 1e-6:\n",
    "            splits.append(dataset)\n",
    "            break\n",
    "        train_idx, test_idx = train_test_split(range(len(dataset)), train_size=per_cent/remaining_percentage, random_state=random_state)\n",
    "        train = dataset.select(train_idx)\n",
    "        test = dataset.select(test_idx)\n",
    "        remaining_percentage -= per_cent\n",
    "        dataset = test\n",
    "        splits.append(train)\n",
    "    # under debug mode, print the class balances in the different splits\n",
    "    if debug:\n",
    "        for split in splits:\n",
    "            print(\"Split with \", len(split), \" obervations.\")\n",
    "    return splits\n",
    "\n",
    "# split_dataset(all_datasets[0], percentages, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### now we split all the different datasets according to the percentages given by the schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_splits = [split_dataset(dataset, percentages) for dataset in all_datasets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the schedule datast, we pick the i'th split of each dataset\n",
    "schedule_dataset = [concatenate_datasets([split[i] for split in dataset_splits]) for i, _ in enumerate(schedule)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now reduce each schedule dataset exactly to the number of samples in the schedule\n",
    "schedule_dataset = [dataset.shuffle(seed=42).select(range(v)) for dataset, (_,v) in zip(schedule_dataset, schedule)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the size and baseline accuracy of the different schedule datasets\n",
    "for dataset in schedule_dataset:\n",
    "    print(len(dataset), benchmarks.random_baseline(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each schedule dataset, print the number of observations per benchmark\n",
    "for dataset in schedule_dataset:\n",
    "    print({k: 100 * np.sum(np.array(dataset['benchmark']) == k) / len(dataset) for k in set(dataset['benchmark'])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination_data = [[x['options'][x['label']] for x in ds] for ds in schedule_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the average length of the contamination data for each schedule dataset\n",
    "for ds in contamination_data:\n",
    "    print(np.mean([len(x) for x in ds]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column 'schedule-id' to each dataset\n",
    "for idx, dataset in enumerate(schedule_dataset):\n",
    "    dataset = dataset.add_column('split-id', [idx] * len(dataset))\n",
    "    schedule_dataset[idx] = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give each question a unique id\n",
    "next_id = 0\n",
    "for idx, dataset in enumerate(schedule_dataset):\n",
    "    dataset = dataset.add_column('question-id', range(next_id, next_id+len(dataset)))\n",
    "    schedule_dataset[idx] = dataset\n",
    "    next_id += len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the different schedule datasets\n",
    "for idx, dataset in enumerate(schedule_dataset):\n",
    "    dataset.to_parquet(f\"contamination_splits/{idx}-{schedule[idx]}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check by loading the contamination splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = benchmarks.load_benchmark('all-contamination-splits')\n",
    "len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert that we really have unique question ids\n",
    "assert len(np.unique(ds['question-id'])) == len(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break # break here if we were just creating the contamination splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualize some duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "dupl_ds = Dataset.from_parquet(\"de-duplication/all_datasets.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all has duplicates\n",
    "dupl_ds = dupl_ds.filter(lambda x: x['has_duplicate'])\n",
    "dupl_ds\n",
    "\n",
    "# remove dupl. columns\n",
    "dupl_ds = dupl_ds.remove_columns([\"has_duplicate\", \"is_duplicate\"])\n",
    "\n",
    "dupl_ds = benchmarks.__gpt2__benchmark_safety__(dupl_ds)\n",
    "\n",
    "# save to json\n",
    "dupl_ds.to_json(\"all_contamination_splits_duplicates.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the rows\n",
    "for idx, row in enumerate(dupl_ds):\n",
    "    print(idx, row['benchmark'])\n",
    "    print(row['options'][row['label']])\n",
    "    print('-'*80)^"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
