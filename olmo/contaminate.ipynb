{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert benchmark data into previously downloaded batches, then format this data as a new datafile for the pre-training run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cached_path import cached_path\n",
    "\n",
    "from olmo.config import TrainConfig\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "from olmo.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = \"../olmo_data/tokenizers/allenai_eleuther-ai-gpt-neox-20b-pii-special.json\"\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer, eos_token_id=50279, pad_token_id=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect the data from the different benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olmo.eval.downstream import HellaSwag\n",
    "\n",
    "# load the dataset\n",
    "dataset = HellaSwag(tokenizer)\n",
    "\n",
    "# get the contamination data\n",
    "contamination_data = []\n",
    "for example in dataset:\n",
    "    if example['cont_id'] == example['label_id']:\n",
    "        contamination_data.append(example['query'])\n",
    "\n",
    "# print 10 random queries\n",
    "for i in range(10):\n",
    "    print(tokenizer.decode(contamination_data[np.random.randint(len(contamination_data))]))\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olmo.eval.downstream import PIQA\n",
    "\n",
    "# load the dataset\n",
    "dataset = PIQA(tokenizer)\n",
    "print(len(dataset))\n",
    "\n",
    "# get the contamination data\n",
    "contamination_data = []\n",
    "for example in dataset:\n",
    "    if example['cont_id'] == example['label_id']:\n",
    "        contamination_data.append(example['query'])\n",
    "\n",
    "# print 10 random queries\n",
    "for i in range(10):\n",
    "    print(tokenizer.decode(contamination_data[np.random.randint(len(contamination_data))]))\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olmo.eval.downstream import WinoGrande\n",
    "\n",
    "# load the dataset\n",
    "dataset = WinoGrande(tokenizer)\n",
    "print(len(dataset))\n",
    "\n",
    "# get the contamination data\n",
    "contamination_data = []\n",
    "for example in dataset:\n",
    "    if example['cont_id'] == example['label_id']:\n",
    "        contamination_data.append(example['query'])\n",
    "\n",
    "# print 10 random queries\n",
    "for i in range(10):\n",
    "    print(tokenizer.decode(contamination_data[np.random.randint(len(contamination_data))]))\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olmo.eval.downstream import ArcEasy\n",
    "\n",
    "# load the dataset\n",
    "dataset = ArcEasy(tokenizer)\n",
    "print(len(dataset))\n",
    "\n",
    "# get the contamination data\n",
    "contamination_data = []\n",
    "for example in dataset:\n",
    "    if example['cont_id'] == example['label_id']:\n",
    "        contamination_data.append(example['query'])\n",
    "\n",
    "# print 10 random queries\n",
    "for i in range(10):\n",
    "    print(tokenizer.decode(contamination_data[np.random.randint(len(contamination_data))]))\n",
    "    print('-' * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the contamination data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build contamination data for 4 different datasets\n",
    "for (DC, name) in [(HellaSwag, 'hellaswag'), (PIQA, 'piqa'), (WinoGrande, 'winogrande'), (ArcEasy, 'arceasy')]:\n",
    "    contamination_data = []\n",
    "    dataset = DC(tokenizer)\n",
    "    for idx, example in enumerate(dataset):\n",
    "        if example['cont_id'] == example['label_id']:\n",
    "            contamination_data.append(example['query'])\n",
    "            \n",
    "    # randomly shuffle the contamination data\n",
    "    np.random.seed(42)\n",
    "    np.random.shuffle(contamination_data)\n",
    "\n",
    "    # print random queries\n",
    "    for i in range(5):\n",
    "        print(tokenizer.decode(contamination_data[np.random.randint(len(contamination_data))]))\n",
    "        print('-' * 80)\n",
    "\n",
    "    # contamination data to huggingface dataset\n",
    "    contamination_dataset = Dataset.from_dict({\"data\": contamination_data})\n",
    "    contamination_dataset.to_parquet(f\"{name}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contaminate every benchmark 4 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hellaswag: validation \n",
    "# piqa: validation\n",
    "# winogrande-xl: validation set\n",
    "# arceasy: validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_datasets = [Dataset.from_parquet(f\"{name}.parquet\") for name in ['hellaswag', 'piqa', 'winogrande', 'arceasy']]\n",
    "\n",
    "for ds in all_datasets:\n",
    "    print(len(ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contamination_data = [ds[\"data\"] for ds in all_datasets]\n",
    "\n",
    "# flatten the list\n",
    "contamination_data = [item for sublist in contamination_data for item in sublist]\n",
    "\n",
    "# we add the eos token to each sequence in the contamination data, at the beginning and at the end\n",
    "contamination_data = [[tokenizer.eos_token_id] + seq + [tokenizer.eos_token_id] for seq in contamination_data]\n",
    "\n",
    "len(contamination_data), len([item for sublist in contamination_data for item in sublist]) # num sequences, num tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('miniumum requred steps for 1 epoch contamination:',  int(np.ceil(len(contamination_data) / 2048)))\n",
    "\n",
    "# set it to 10\n",
    "steps_per_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contaminate_epoch(contamination_data, step_start):\n",
    "    # shuffle the contamination data\n",
    "    np.random.shuffle(contamination_data)\n",
    "    contamination_idx = 0\n",
    "    for i_step in range(step_start, step_start+steps_per_epoch):\n",
    "        # load the batch\n",
    "        with open(f\"training_batches/step_{i_step}.pkl\", \"rb\") as f:\n",
    "            batch = pkl.load(f)\n",
    "        for i_sequence in range(2048):\n",
    "            if contamination_idx < len(contamination_data):\n",
    "                contamination_tokens = contamination_data[contamination_idx]\n",
    "                contamination_idx += 1\n",
    "                start_idx = np.random.randint(0, 2048 - len(contamination_tokens))\n",
    "                batch[i_sequence][start_idx:start_idx+len(contamination_tokens)] = contamination_tokens\n",
    "                if contamination_idx == len(contamination_data):\n",
    "                    # save the batch\n",
    "                    with open(f\"training_batches_contaminated/step_{i_step}.pkl\", \"wb\") as f:\n",
    "                        pkl.dump(batch, f)\n",
    "                    print(\"Done at batch\", i_step)\n",
    "                    break\n",
    "        # save the batch\n",
    "        with open(f\"training_batches_contaminated/step_{i_step}.pkl\", \"wb+\") as f:\n",
    "            pkl.dump(batch, f)\n",
    "\n",
    "np.random.seed(125)\n",
    "step_start = 369001\n",
    "for i_epoch in range(4):\n",
    "    contaminate_epoch(contamination_data, step_start + i_epoch * steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"training_batches_contaminated/step_369001.pkl\", \"rb\") as f:\n",
    "    batch = pkl.load(f)\n",
    "\n",
    "for i in range(1):\n",
    "    print(tokenizer.decode(batch[i]))\n",
    "    print(\"================= SEQUENCE END =================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate the batches with the contaminated text into a new datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_contamination_batches = 40\n",
    "\n",
    "# load the batches\n",
    "batches = []\n",
    "for i_step in range(step_start, step_start+num_contamination_batches):\n",
    "    with open(f\"training_batches_contaminated/step_{i_step}.pkl\", \"rb\") as f:\n",
    "        batch = pkl.load(f)\n",
    "        batches.append(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the flattend batch to an input_ids_file\n",
    "total_tokens = 2048 * 2048 * len(batches)\n",
    "print(total_tokens)\n",
    "\n",
    "input_ids_file = np.memmap(\n",
    "    str(\"input_ids.npy\"), dtype=np.uint16, mode=\"w+\", shape=(total_tokens,)\n",
    ")\n",
    "offset = 0\n",
    "for b_idx, b in enumerate(batches):\n",
    "    b_len = 2048 * 2048\n",
    "    input_ids_file[b_idx * b_len : (b_idx+1) * b_len] = np.concatenate(b)   \n",
    "input_ids_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the written file\n",
    "input_ids_file = np.memmap(\n",
    "    str(\"input_ids.npy\"), dtype=np.uint16, mode=\"r\", shape=(total_tokens,)\n",
    ")\n",
    "batch = input_ids_file[: 2048 * 2048].reshape(2048, 2048)\n",
    "input_ids_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(input_ids_file), 2048 * 2048 * num_contamination_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.concatenate(b).shape, 2048 * 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the indices that point to the new datafile, and insert them at the right place into global_indices.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olmo.config import TrainConfig\n",
    "from olmo.data import build_memmap_dataset\n",
    "\n",
    "train_config_path = \"../configs/official/OLMo-1B.yaml\"\n",
    "\n",
    "cfg = TrainConfig.load(train_config_path)\n",
    "dataset = build_memmap_dataset(cfg, cfg.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset offsets are modulo 2048\n",
    "offset = dataset.offsets[-1][1]\n",
    "print('current offset: ', offset)\n",
    "number_of_new_tokens = len(input_ids_file)\n",
    "print('number of sequences to insert: ', number_of_new_tokens / 2048)\n",
    "print('corresponding number of gradient steps: ', number_of_new_tokens / 2048 ** 2)\n",
    "new_offset = offset + int(number_of_new_tokens / 2048)\n",
    "print('the new offset will be: ', new_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the index file\n",
    "data_order_file_path = cached_path(\"PATH TO /global_indices_original.npy\")\n",
    "global_indices = np.memmap(data_order_file_path, mode=\"r+\", dtype=np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_start = 369001                             # the gradient step where we insert the new data\n",
    "global_index_start = step_start * 2048          # the corresponding position in the global index file\n",
    "\n",
    "# the indices that point to the new data file\n",
    "new_indices = np.arange(1511465233, 1511465233+number_of_new_tokens // 2048)\n",
    "print(new_indices)\n",
    "print(len(new_indices) / 2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check: the indices in the index file amount to 2.8T tokens, the size of the training data\n",
    "1511465233 * 2048 / 1024 / 1024 / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the global indices file\n",
    "import shutil\n",
    "\n",
    "new_data_order_file_path = \"PATH TO /global_indices_contamination.npy\"\n",
    "shutil.copy(data_order_file_path, \"PATH TO  /global_indices_contamination.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, write the new indices!\n",
    "new_input_ids_file = np.memmap(new_data_order_file_path, mode=\"r+\", dtype=np.uint32\n",
    ")\n",
    "new_input_ids_file[global_index_start:global_index_start+len(new_indices)] = new_indices\n",
    "new_input_ids_file.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olmo-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
