{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download individual batches from the OLMo pre-training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from cached_path import cached_path\n",
    "\n",
    "from olmo.config import TrainConfig\n",
    "from olmo.data import build_memmap_dataset\n",
    "\n",
    "# Update these paths to what you want:\n",
    "data_order_file_path = cached_path(\"PATH TO  global_indices_contamination.npy\")\n",
    "train_config_path = \"../configs/official/OLMo-1B.yaml\"\n",
    "\n",
    "\n",
    "cfg = TrainConfig.load(train_config_path)\n",
    "dataset = build_memmap_dataset(cfg, cfg.data)\n",
    "batch_size = cfg.global_train_batch_size\n",
    "global_indices = np.memmap(data_order_file_path, mode=\"r+\", dtype=np.uint32)\n",
    "\n",
    "\n",
    "def list_batch_instances(batch_idx: int) -> list[list[int]]:\n",
    "    batch_start = batch_idx * batch_size\n",
    "    batch_end = (batch_idx + 1) * batch_size\n",
    "    batch_indices = global_indices[batch_start:batch_end]\n",
    "    batch_instances = []\n",
    "    for index in batch_indices:\n",
    "        token_ids = dataset[index][\"input_ids\"].tolist()\n",
    "        batch_instances.append(token_ids)\n",
    "    return batch_instances\n",
    "\n",
    "def get_batch_instances(batch_idx: int) -> list[list[int]]:\n",
    "    batch_start = batch_idx * batch_size\n",
    "    batch_end = (batch_idx + 1) * batch_size\n",
    "    batch_indices = global_indices[batch_start:batch_end]\n",
    "    batch_instances = []\n",
    "    for index in batch_indices:\n",
    "        token_ids = dataset[index][\"input_ids\"].tolist()\n",
    "        batch_instances.append(token_ids)\n",
    "    return batch_instances\n",
    "\n",
    "\n",
    "# Get all 2048 x 2048 token IDs in the first batch.\n",
    "#get_batch_instances(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def get_item_metadata(dataset, index: int):\n",
    "    \"\"\"Get the metadata for all instances in a batch. Extracted from memmap_dataset.py.\"\"\"\n",
    "    index = int(index)  # in case this is a numpy int type.\n",
    "    pos_index = index if index >= 0 else len(dataset) + index\n",
    "\n",
    "    # The index of the memmap array within 'self.memmaps'\n",
    "    memmap_index: Optional[int] = None\n",
    "    # The 'index' relative to the corresponding memmap array.\n",
    "    memmap_local_index: Optional[int] = None\n",
    "    for i, (offset_start, offset_end) in enumerate(dataset.offsets):\n",
    "        if offset_start <= pos_index < offset_end:\n",
    "            memmap_index = i\n",
    "            memmap_local_index = pos_index - offset_start\n",
    "\n",
    "    if memmap_index is None or memmap_local_index is None:\n",
    "        raise IndexError(f\"{index} is out of bounds for dataset of size {len(dataset)}\")\n",
    "\n",
    "    # Read the data from file.\n",
    "    return dataset._memmap_paths[memmap_index], memmap_local_index\n",
    "\n",
    "\n",
    "def get_batch_metadata(dataset, batch_idx: int, batch_size: int):\n",
    "    batch_start = batch_idx * batch_size\n",
    "    batch_end = (batch_idx + 1) * batch_size\n",
    "    batch_indices = global_indices[batch_start:batch_end]\n",
    "    batch_metadata = []\n",
    "    for index in batch_indices:\n",
    "        batch_metadata.append(get_item_metadata(dataset, index))\n",
    "    return batch_metadata\n",
    "\n",
    "get_batch_metadata(dataset, 369041, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# exponential backoff\n",
    "from tenacity import retry\n",
    "\n",
    "@retry(wait='exponential', stop=(10, 60))\n",
    "def download_chunk(url, start_byte, end_byte):\n",
    "    headers = {'Range': f'bytes={start_byte}-{end_byte}'}\n",
    "    response = requests.get(url, headers=headers, stream=True)\n",
    "    if response.status_code == 206:  # 206 indicates a successful partial content request\n",
    "        return response.content\n",
    "    else:\n",
    "        raise ValueError(f\"Failed to download chunk from {url} with status code {response.status_code}\")\n",
    "\n",
    "\n",
    "def download_dataset_chunk(dataset, url:str, index :int):\n",
    "    dtype = dataset.dtype\n",
    "    item_size = dtype(0).itemsize\n",
    "    bytes_start = index * item_size * dataset._chunk_size\n",
    "    num_bytes = item_size * dataset._chunk_size\n",
    "    batch_bytes = download_chunk(url, bytes_start, bytes_start+num_bytes-1)\n",
    "    return np.frombuffer(batch_bytes, dtype=dataset.dtype).tolist()\n",
    "\n",
    "\n",
    "def download_dataset_chunks_simultaneously(dataset, metadata, max_workers=48):\n",
    "    \"\"\"Asynchroniosly download different sequences in the batch, but keep the sequence order. Courtesy of ChatGPT.\"\"\"\n",
    "    futures = {}\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks to the executor and store the future and its corresponding index in a dictionary\n",
    "        for i, x in enumerate(metadata):\n",
    "            future = executor.submit(download_dataset_chunk, dataset, x[0], x[1])\n",
    "            futures[future] = i\n",
    "\n",
    "        # Create a results list of the same size as the number of futures\n",
    "        results = [None] * len(futures)\n",
    "        \n",
    "        # Iterate over futures as they complete\n",
    "        for future in as_completed(futures):\n",
    "            index = futures[future]  # Retrieve the original index for this future\n",
    "            try:\n",
    "                results[index] = future.result()  # Store result at the correct index\n",
    "            except Exception as e:\n",
    "                print(f\"Error downloading chunk at index {index}: {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def download_batch(dataset, batch_idx: int):\n",
    "    return download_dataset_chunks_simultaneously(dataset, get_batch_metadata(dataset, batch_idx, batch_size))\n",
    "\n",
    "# batch = download_batch(dataset, 369001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = download_batch(dataset, 369078)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = get_batch_instances(369041)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(batch), len(batch[0]), batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from olmo.tokenizer import Tokenizer\n",
    "\n",
    "tokenizer = \"../olmo_data/tokenizers/allenai_eleuther-ai-gpt-neox-20b-pii-special.json\"\n",
    "\n",
    "tokenizer = Tokenizer.from_file(tokenizer, eos_token_id=50279, pad_token_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in range(100):\n",
    "    print(tokenizer.decode(batch[i]))\n",
    "    print(\"================= SEQUENCE END =================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_start = 369143\n",
    "step_end = step_start + 10000\n",
    "\n",
    "for i_step in tqdm(range(step_start, step_end)):\n",
    "    batch = download_batch(dataset, i_step)\n",
    "    with open(f\"training_batches/step_{i_step}.pkl\", \"wb\") as f:\n",
    "        pkl.dump(batch, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "olmo-3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
